{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90791,"databundleVersionId":10592855,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#importing all the necessary and allowed libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:44:56.502742Z","iopub.execute_input":"2025-03-21T02:44:56.503046Z","iopub.status.idle":"2025-03-21T02:44:57.501922Z","shell.execute_reply.started":"2025-03-21T02:44:56.503026Z","shell.execute_reply":"2025-03-21T02:44:57.500651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Introduction to the Problem\nThe goal of this model is to predict a systemâ€™s probability of getting infected by various families of malware, based on different properties of that system. The telemetry data containing these properties and the system infections was generated by threat reports collected by system's antivirus software.\n\n# Data Loading\nWe will now load the dataset and inspect to make sure that everything is loaded correctly like all the columns are present etc","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/System-Threat-Forecaster'\n\ntrain = pd.read_csv('/kaggle/input/System-Threat-Forecaster/train.csv')\ntest = pd.read_csv('/kaggle/input/System-Threat-Forecaster/test.csv')\nsub = pd.read_csv('/kaggle/input/System-Threat-Forecaster/sample_submission.csv')\n\ntrain.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:44:57.503118Z","iopub.execute_input":"2025-03-21T02:44:57.503536Z","iopub.status.idle":"2025-03-21T02:44:58.888660Z","shell.execute_reply.started":"2025-03-21T02:44:57.503511Z","shell.execute_reply":"2025-03-21T02:44:58.887739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nFeature types and number of missing values, we will impute these when we make preprocessing pipelines.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:44:58.890126Z","iopub.execute_input":"2025-03-21T02:44:58.890406Z","iopub.status.idle":"2025-03-21T02:44:59.018796Z","shell.execute_reply.started":"2025-03-21T02:44:58.890376Z","shell.execute_reply":"2025-03-21T02:44:59.017793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(train.isnull().mean() * 100).sort_values(ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:44:59.020149Z","iopub.execute_input":"2025-03-21T02:44:59.020406Z","iopub.status.idle":"2025-03-21T02:44:59.122659Z","shell.execute_reply.started":"2025-03-21T02:44:59.020383Z","shell.execute_reply":"2025-03-21T02:44:59.121154Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since the number of missing values is vert few we will handle missing values by imputing as opposed to dropping since we can preserve rows by doing so.\n\n# Feature Engineering\n\nHere \n'DateAS' : Malware signature dates ,\n'DateOS' : timestamps for OSVersion which gives the time that the OS was last updated\n\nThe new feature represents the age of new system's age after updating","metadata":{}},{"cell_type":"code","source":"train['DateAS'] = pd.to_datetime(train['DateAS'])\ntrain['DateOS'] = pd.to_datetime(train['DateOS'])\ntrain['SystemAge'] = (train['DateAS'] - train['DateOS']).dt.days\n\ntest['DateAS'] = pd.to_datetime(test['DateAS'])\ntest['DateOS'] = pd.to_datetime(test['DateOS'])\ntest['SystemAge'] = (test['DateAS'] - test['DateOS']).dt.days\n\ntrain.drop(columns=['DateAS'], inplace=True)\ntest.drop(columns=['DateAS'], inplace=True)\ntrain.drop(columns=['DateOS'], inplace=True)\ntest.drop(columns=['DateOS'], inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:44:59.123917Z","iopub.execute_input":"2025-03-21T02:44:59.124231Z","iopub.status.idle":"2025-03-21T02:44:59.215020Z","shell.execute_reply.started":"2025-03-21T02:44:59.124205Z","shell.execute_reply":"2025-03-21T02:44:59.213899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#violin plot of the new dataset\nsns.violinplot(x='SystemAge', data = train)\nplt.xlabel('SystemAge')\nplt.title('Violin Plot of SystemAge')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:44:59.216120Z","iopub.execute_input":"2025-03-21T02:44:59.216449Z","iopub.status.idle":"2025-03-21T02:44:59.595237Z","shell.execute_reply.started":"2025-03-21T02:44:59.216419Z","shell.execute_reply":"2025-03-21T02:44:59.594089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Such a distribution indicates popular chasis' probably had some new update at specific times quite close to the date that the data was collected.\n\nNote that a negative age indicates that a malware was detected before the OS was updated, although we do not find as many instances of this occuring. (approximately 1.5%)","metadata":{}},{"cell_type":"code","source":"(train['SystemAge'].value_counts()<0).count()/len(train['SystemAge'])\n#dont need to check for null values since we know there are none","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:44:59.596088Z","iopub.execute_input":"2025-03-21T02:44:59.596368Z","iopub.status.idle":"2025-03-21T02:44:59.606933Z","shell.execute_reply.started":"2025-03-21T02:44:59.596342Z","shell.execute_reply":"2025-03-21T02:44:59.605924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['target'].value_counts().plot(kind='bar')\nplt.title(\"Distribution of Target\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:44:59.608935Z","iopub.execute_input":"2025-03-21T02:44:59.609189Z","iopub.status.idle":"2025-03-21T02:44:59.779627Z","shell.execute_reply.started":"2025-03-21T02:44:59.609165Z","shell.execute_reply":"2025-03-21T02:44:59.778899Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Dataset is fairly balenced doesnt require SMOTE","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x=train['ProcessorCoreCount'])\nplt.title(\"ProcessorCoreCount Boxplot\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:44:59.780436Z","iopub.execute_input":"2025-03-21T02:44:59.780634Z","iopub.status.idle":"2025-03-21T02:44:59.892177Z","shell.execute_reply.started":"2025-03-21T02:44:59.780611Z","shell.execute_reply":"2025-03-21T02:44:59.891231Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most machines have 2 or 4 cores (which is typical for consumer devices) quite a few have 12 and some upto 60","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='IsSystemProtected', hue='target', data=train)\nplt.title(\"IsSystemProtected vs Target\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:44:59.892831Z","iopub.execute_input":"2025-03-21T02:44:59.893077Z","iopub.status.idle":"2025-03-21T02:45:00.044605Z","shell.execute_reply.started":"2025-03-21T02:44:59.893056Z","shell.execute_reply":"2025-03-21T02:45:00.043527Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most machines have some sort of protection software intalled. Unusually, protected systems have a slightly higher rate of getting infected.\n","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='HasOpticalDiskDrive', hue='target', data=train)\nplt.title(\"HasOpticalDiskDrive vs Target\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:45:00.045662Z","iopub.execute_input":"2025-03-21T02:45:00.046027Z","iopub.status.idle":"2025-03-21T02:45:00.175861Z","shell.execute_reply.started":"2025-03-21T02:45:00.045992Z","shell.execute_reply":"2025-03-21T02:45:00.175000Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Most machines don't have an optical disk drive.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='target', y='TotalPhysicalRAMMB', data=train)\nplt.title(\"TotalPhysicalRAMMB vs Target\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:45:42.112276Z","iopub.execute_input":"2025-03-21T02:45:42.112613Z","iopub.status.idle":"2025-03-21T02:45:42.259648Z","shell.execute_reply.started":"2025-03-21T02:45:42.112585Z","shell.execute_reply":"2025-03-21T02:45:42.258440Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"top_versions = train['EngineVersion'].value_counts().nlargest(10).index\nsns.countplot(x='EngineVersion', data=train[train['EngineVersion'].isin(top_versions)])\nplt.title(\"Top 10 EngineVersion\")\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:46:05.321021Z","iopub.execute_input":"2025-03-21T02:46:05.321403Z","iopub.status.idle":"2025-03-21T02:46:05.531729Z","shell.execute_reply.started":"2025-03-21T02:46:05.321374Z","shell.execute_reply":"2025-03-21T02:46:05.530717Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"most of the engine versions are ```1.1.15200.1``` and ```1.1.15100.1```","metadata":{}},{"cell_type":"code","source":"top_10_countries = train['CountryID'].value_counts().nlargest(10).index\nsns.countplot(x='CountryID', data=train[train['CountryID'].isin(top_10_countries)])\nplt.title(\"Top 10 CountryID Distribution\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:46:54.934665Z","iopub.execute_input":"2025-03-21T02:46:54.935106Z","iopub.status.idle":"2025-03-21T02:46:55.196272Z","shell.execute_reply.started":"2025-03-21T02:46:54.935066Z","shell.execute_reply":"2025-03-21T02:46:55.195298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['ChassisType'].value_counts()\n#most machines are Notebooks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# dropping columns that dont have more than one unique values\ncols_to_drop = ['IsBetaUser', 'AutoSampleSubmissionEnabled', 'IsFlightsDisabled', 'MachineID']\ntrain.drop(columns=cols_to_drop, inplace=True)\ntest.drop(columns=cols_to_drop, inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:49:55.734041Z","iopub.execute_input":"2025-03-21T02:49:55.734390Z","iopub.status.idle":"2025-03-21T02:49:55.758273Z","shell.execute_reply.started":"2025-03-21T02:49:55.734352Z","shell.execute_reply":"2025-03-21T02:49:55.757568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cheking if the columns are dropped\ntrain.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:50:11.116804Z","iopub.execute_input":"2025-03-21T02:50:11.117245Z","iopub.status.idle":"2025-03-21T02:50:11.124218Z","shell.execute_reply.started":"2025-03-21T02:50:11.117210Z","shell.execute_reply":"2025-03-21T02:50:11.122926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n#replaces all categories but top n (based on frequency) to 'Other' \nclass TopNEncoder(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, n):\n        self.n = n\n        self.top_n_ = None\n\n    def fit(self, X, y=None):\n        X = self._convert_to_series(X)\n        freq = X.value_counts()\n        self.top_n_ = freq.nlargest(self.n).index\n        return self\n\n    def transform(self, X):\n        X = self._convert_to_series(X)\n        return X.where(X.isin(self.top_n_), other='Other').values.reshape(-1, 1)\n\n    def _convert_to_series(self, X):\n        \"\"\"\n        Converts X to a one-dimensional pandas Series.\n        If X is (n_samples, 1) or (n_samples,), make it a Series.\n        \"\"\"\n        if isinstance(X, pd.DataFrame):\n            if X.shape[1] != 1:\n                raise ValueError(\"TopNEncoder expects a single column DataFrame.\")\n            X = X.iloc[:, 0]  # get the single column as a Series\n        elif isinstance(X, np.ndarray):\n            # Flatten if it's (n_samples, 1)\n            if len(X.shape) == 2 and X.shape[1] == 1:\n                X = X.ravel()\n            # Now X is shape (n_samples,)\n            X = pd.Series(X)\n        else:\n            # If it's already a Series or some other type, you may need more checks\n            # but typically Series or array are the main cases.\n            pass\n        return X\n\ntop_n_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('top_n', TopNEncoder(n=10)),  # or n=15, etc.\n    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:08:38.903436Z","iopub.execute_input":"2025-03-21T03:08:38.903758Z","iopub.status.idle":"2025-03-21T03:08:38.911008Z","shell.execute_reply.started":"2025-03-21T03:08:38.903736Z","shell.execute_reply":"2025-03-21T03:08:38.910066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nnumeric_median_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncat_mode_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore', drop='first'))\n])\n\nbinary_cat_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:09:45.186495Z","iopub.execute_input":"2025-03-21T03:09:45.186881Z","iopub.status.idle":"2025-03-21T03:09:45.192406Z","shell.execute_reply.started":"2025-03-21T03:09:45.186848Z","shell.execute_reply":"2025-03-21T03:09:45.191155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FrequencyEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.freq_map_ = {}\n\n    def fit(self, X, y=None):\n        X = self._convert_to_series(X)\n        freq = X.value_counts() / len(X)\n        self.freq_map_ = freq.to_dict()\n        return self\n\n    def transform(self, X):\n        X = self._convert_to_series(X)\n        return X.map(self.freq_map_).fillna(0).values.reshape(-1, 1)\n\n    def _convert_to_series(self, X):\n        if isinstance(X, pd.DataFrame):\n            if X.shape[1] != 1:\n                raise ValueError(\"FrequencyEncoder expects a single column.\")\n            X = X.iloc[:, 0]\n        elif isinstance(X, np.ndarray):\n            if len(X.shape) == 2 and X.shape[1] == 1:\n                X = X.ravel()\n            X = pd.Series(X)\n        return X\n\nid_freq_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n    ('freq_encoder', FrequencyEncoder())\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:10:11.096432Z","iopub.execute_input":"2025-03-21T03:10:11.096767Z","iopub.status.idle":"2025-03-21T03:10:11.104106Z","shell.execute_reply.started":"2025-03-21T03:10:11.096738Z","shell.execute_reply":"2025-03-21T03:10:11.102768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transformers = [] \nnumeric_cols_median = [\n    'NumAntivirusProductsInstalled',\n    'SystemAge',\n    'OSBuildRevisionOnly',\n    'PrimaryDisplayResolutionVertical',\n    'SystemVolumeCapacityMB',\n    'PrimaryDisplayResolutionHorizontal',\n    'OSBuildNumber',\n    'PrimaryDisplayDiagonalInches',\n    'OSBuildNumberOnly',\n    'PrimaryDiskCapacityMB',\n    'ProcessorCoreCount',\n    'TotalPhysicalRAMMB',\n    'InternalBatteryNumberOfCharges',\n    'NumAntivirusProductsEnabled',\n]\n\nordinal = [\n    \n    \n]\n\nbinary_cols = [\n    'IsAlwaysOnAlwaysConnectedCapable',\n    'IsTouchEnabled',\n    'IsPassiveModeEnabled',\n    'IsVirtualDevice',\n    'IsPenCapable',\n    'SMode',\n    'IsSecureBootEnabled',\n    'IsPortableOS',\n    'HasTpm',\n    'FirewallEnabled',\n    'HasOpticalDiskDrive',\n    'IsGamer',\n    'IsSystemProtected',\n    'DeviceFamily',\n    'ProductName'  #only has two values\n]\n\n\nid_type = [\n    'FirmwareManufacturerID',\n    'OEMNameID',\n    'OEMModelID',\n    'FirmwareVersionID',\n    'LocaleEnglishNameID',\n    'IEVersionID',\n    'ProcessorModelID',\n    'AntivirusConfigID',\n    'OSInstallLanguageID',\n    'OSUILocaleID'\n]\n\nfor col in id_type:\n    train[col] = train[col].astype(str)\n    test[col] = test[col].astype(str)\nfor col in id_type:\n    transformers.append(\n        (f\"id_freq_{col}\", id_freq_pipeline, [col])\n    )\n\n\ncat_mode = [\n    'OSProductSuite',\n    'EnableLUA', # 3 unique values only\n    'ProcessorManufacturerID', #has very few (4) unique values\n    'RealTimeProtectionState',\n    'FlightRing',\n    'OSVersion',\n    'OSEdition',\n    'OSBranch',\n    'LicenseActivationChannel',\n    'OsPlatformSubRelease',\n    'PowerPlatformRole',\n    'Processor',\n    'AutoUpdateOptionsName',\n    'PrimaryDiskType',\n    'OSSkuFriendlyName',\n    'OSArchitecture',\n    'OSInstallType',\n    'MDC2FormFactor',\n]\n\n#selected columns have very few values in the class of <top n categories\ntop_n = [\n    'CityID',\n    'NumericOSVersion',\n    'SignatureVersion',\n    'GeoRegionID',\n    'OSBuildLab',\n    'EngineVersion',\n    'OSGenuineState',   \n    'CountryID',\n    'PlatformType',     \n    'SKUEditionName',   \n    'AppVersion',\n    'ChassisType'\n]\n\n#converting top- n columns to strings\n\nfor col in top_n:\n    train[col] = train[col].astype(str)\n    test[col] = test[col].astype(str)\n\n#add the top-n encoding pipeline for each column\nfor col in top_n:\n    transformers.append(\n        (f\"top_n_{col}\", top_n_pipeline, [col])\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:10:13.291048Z","iopub.execute_input":"2025-03-21T03:10:13.291400Z","iopub.status.idle":"2025-03-21T03:10:13.810303Z","shell.execute_reply.started":"2025-03-21T03:10:13.291372Z","shell.execute_reply":"2025-03-21T03:10:13.808961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['ProcessorManufacturerID'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:22:57.137123Z","iopub.execute_input":"2025-03-21T03:22:57.137443Z","iopub.status.idle":"2025-03-21T03:22:57.145387Z","shell.execute_reply.started":"2025-03-21T03:22:57.137417Z","shell.execute_reply":"2025-03-21T03:22:57.144580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n    ('num_median', numeric_median_pipeline, numeric_cols_median),\n    ('binary_cat', binary_cat_pipeline, binary_cols),\n    ('cat_mode', cat_mode_pipeline, cat_mode),\n    # ('top_n_cat', top_n_pipeline, top_n)\n    ] + transformers,\n    remainder='drop'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:11:13.321748Z","iopub.execute_input":"2025-03-21T03:11:13.322145Z","iopub.status.idle":"2025-03-21T03:11:13.355410Z","shell.execute_reply.started":"2025-03-21T03:11:13.322108Z","shell.execute_reply":"2025-03-21T03:11:13.354231Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fitting preprocessor and verifying","metadata":{}},{"cell_type":"code","source":"# Separate target from training\ny_train = train['target']\nX_train = train.drop(columns=['target'])\n\n# Also drop 'target' from test if it exists\n# (sometimes test doesn't have target at all)\nX_test = test.drop(columns=['target'], errors='ignore')\n\n# Now fit/transform on X only\nX_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:33:25.016626Z","iopub.execute_input":"2025-03-21T03:33:25.017027Z","iopub.status.idle":"2025-03-21T03:33:27.555330Z","shell.execute_reply.started":"2025-03-21T03:33:25.016992Z","shell.execute_reply":"2025-03-21T03:33:27.554171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The warning is just a heads up that the model is encountering new categories at the inference time, this is usually okay to leave as is. We can check the size of the new dataset just to be sure though.","metadata":{}},{"cell_type":"code","source":"print(train.shape, X_train_processed.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:33:30.976408Z","iopub.execute_input":"2025-03-21T03:33:30.976712Z","iopub.status.idle":"2025-03-21T03:33:30.982222Z","shell.execute_reply.started":"2025-03-21T03:33:30.976687Z","shell.execute_reply":"2025-03-21T03:33:30.981011Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have learnt how one hot encoders work and blow up the size of the data, so this is expected. Lets verify this using a calculation","metadata":{}},{"cell_type":"code","source":"#Inspect how many columns each part produces\n\ntotal_cols = 0\nfor name, trans, cols in preprocessor.transformers_:\n    # Skip dropped or empty groups\n    if trans == 'drop' or len(cols) == 0:\n        continue\n\n    # Extract just the columns handled by this transformer\n    subset = X_train[cols]\n\n    # Transform that subst alone (the pipeline/transform is already fitted)\n    subset_t = trans.transform(subset)\n\n    # Count how many columns we now have\n    n_cols = subset_t.shape[1]\n    total_cols += n_cols\n\n    print(f\"Transformer '{name}' on columns {cols} -> {n_cols} columns\")\n\nprint(f\"\\nSum of columns from all transformers: {total_cols}\")\nprint(f\"Columns in final output: {X_train_processed.shape[1]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:39:00.545537Z","iopub.execute_input":"2025-03-21T03:39:00.545913Z","iopub.status.idle":"2025-03-21T03:39:01.850319Z","shell.execute_reply.started":"2025-03-21T03:39:00.545879Z","shell.execute_reply":"2025-03-21T03:39:01.849291Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since the expected number of columns matches the resultant, we can now move forward","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train_part, X_val_part, y_train_part, y_val_part = train_test_split(\n    X_train_processed,  # not the raw X_train\n    y_train,\n    test_size=0.2,\n    random_state=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:40:40.971485Z","iopub.execute_input":"2025-03-21T03:40:40.971787Z","iopub.status.idle":"2025-03-21T03:40:41.005399Z","shell.execute_reply.started":"2025-03-21T03:40:40.971763Z","shell.execute_reply":"2025-03-21T03:40:41.003668Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Looking for null values","metadata":{}},{"cell_type":"code","source":"df_part = pd.DataFrame(X_train_part)\nprint(df_part.isnull().sum())  # check for NaNs in each column\nprint(\"Rows with NaN:\", df_part.isnull().any(axis=1).sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:41:52.089388Z","iopub.execute_input":"2025-03-21T03:41:52.089699Z","iopub.status.idle":"2025-03-21T03:41:54.462899Z","shell.execute_reply.started":"2025-03-21T03:41:52.089677Z","shell.execute_reply":"2025-03-21T03:41:54.461782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"## Random Forest\nSince we have tabular data we will start off by Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.metrics import confusion_matrix\n\nclf = RandomForestClassifier(random_state=1)\nclf.fit(X_train_part, y_train_part)\ny_pred = clf.predict(X_val_part)\n\nprint(\"Accuracy:\", accuracy_score(y_val_part, y_pred))\nprint(classification_report(y_val_part, y_pred))\nsns.heatmap(confusion_matrix(y_val_part, y_pred), annot=True, cmap='Blues', fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:45:07.897613Z","iopub.execute_input":"2025-03-21T03:45:07.897948Z","iopub.status.idle":"2025-03-21T03:49:08.400281Z","shell.execute_reply.started":"2025-03-21T03:45:07.897924Z","shell.execute_reply":"2025-03-21T03:49:08.399473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n\nlr_clf = LogisticRegression(random_state=1, max_iter=2000)\nlr_clf.fit(X_train_part, y_train_part)\n\ny_pred_lr = lr_clf.predict(X_val_part)\n\nprint(\"Logistic Regression Accuracy:\", accuracy_score(y_val_part, y_pred_lr))\nplt.figure(figsize=(6, 4))\nsns.heatmap(confusion_matrix(y_val_part, y_pred_lr), annot=True, cmap='Blues', fmt='g')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:51:07.276892Z","iopub.execute_input":"2025-03-21T03:51:07.277289Z","iopub.status.idle":"2025-03-21T03:51:19.275933Z","shell.execute_reply.started":"2025-03-21T03:51:07.277261Z","shell.execute_reply":"2025-03-21T03:51:19.275008Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Hyperparametertuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define a parameter grid\nparam_grid = {\n    'penalty': ['l1', 'l2'],\n    'C': [0.01, 0.1, 1, 10, 100],\n    # note that lbfgs doesn't support l1 penalty, so we stick to liblinear or saga\n    'solver': ['liblinear', 'saga']\n}\n\n# Initialize Logistic Regression (no fixed hyperparameters yet)\nlr_base = LogisticRegression(max_iter=2000, random_state=1)\n\n# Set up GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=lr_base,\n    param_grid=param_grid,\n    scoring='accuracy',  # since the competition takes accuracy :)\n    cv=5,                # 5-fold cross-validation\n    n_jobs=-1            # Use all available CPU cores\n)\n\n# Fit the grid search on the training data\ngrid_search.fit(X_train_part, y_train_part)\n\n# Check best parameters and best CV score\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best CV Score:\", grid_search.best_score_)\n\n# Evaluate on the validation set\nbest_lr = grid_search.best_estimator_\ny_pred = best_lr.predict(X_val_part)\n\nprint(\"\\nValidation Accuracy:\", accuracy_score(y_val_part, y_pred))\nprint(classification_report(y_val_part, y_pred))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_val_part, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:22:25.757498Z","iopub.execute_input":"2025-03-21T08:22:25.757830Z","iopub.status.idle":"2025-03-21T08:22:25.770935Z","shell.execute_reply.started":"2025-03-21T08:22:25.757804Z","shell.execute_reply":"2025-03-21T08:22:25.769812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Create the LightGBM model with best params\nlgb_best = lgb.LGBMClassifier(\n    learning_rate=0.1,\n    max_depth=-1,\n    n_estimators=100,\n    num_leaves=31,\n    random_state=42\n)\n\n# Fit the model on your training split\nlgb_best.fit(X_train_part, y_train_part)\n\n# Predict on validation\ny_pred_val = lgb_best.predict(X_val_part)\n\n# Evaluate\nval_acc = accuracy_score(y_val_part, y_pred_val)\nprint(\"Validation Accuracy (LightGBM best params):\", val_acc)\nprint(\"Classification Report:\")\nprint(classification_report(y_val_part, y_pred_val))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_val_part, y_pred_val))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\n# Training XGBoost classifier\nxgb_clf = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\nxgb_clf.fit(X_train_part, y_train_part)\n\n# Make predictions on the validation set\ny_pred_xgb = xgb_clf.predict(X_val_part)\n\n# Print accuracy and classification report\nprint(\"XGBoost Accuracy:\", accuracy_score(y_val_part, y_pred_xgb))\nprint(\"Classification Report:\")\nprint(classification_report(y_val_part, y_pred_xgb))\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_val_part, y_pred_xgb)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Plot the confusion matrix with a blue colormap\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix for XGBoost\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Stacking","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\nxgb_clf = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\nlgb_clf = lgb.LGBMClassifier(random_state=42)\n\nstack_clf = StackingClassifier(\n    estimators=[\n        ('xgb', xgb_clf),\n        ('lgb', lgb_clf)\n    ],\n    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n    cv=3  # 3-fold cross-validation for base estimators\n)\n\n# Fit the stacking model on your training split\nstack_clf.fit(X_train_part, y_train_part)\n\n# Predict on validation\ny_pred_stack = stack_clf.predict(X_val_part)\n\n# Evaluate\nacc_stack = accuracy_score(y_val_part, y_pred_stack)\nprint(\"Stacking (XGB + LGB) Accuracy:\", acc_stack)\nprint(\"Classification Report (Stacking):\")\nprint(classification_report(y_val_part, y_pred_stack))\n\nprint(\"Confusion Matrix (Stacking):\")\nprint(confusion_matrix(y_val_part, y_pred_stack))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter tuning best Model","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\n\n# Baseline default parameters for LightGBM\ndefault_params = {\n    'learning_rate': 0.1,\n    'n_estimators': 100,\n    'num_leaves': 31,\n    'max_depth': -1\n}\n\n# Initialize the LightGBM classifier with default parameters\nlgb_clf = lgb.LGBMClassifier(**default_params)\n\n# Define the parameter grid including the default values\nparam_grid = {\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [100, 200, 500],\n    'num_leaves': [31, 40, 50],\n    'max_depth': [-1, 10, 20, 30]\n}\n\n# Set up GridSearchCV (5-fold cross-validation, using accuracy for scoring)\ngrid_search = GridSearchCV(estimator=lgb_clf,\n                           param_grid=param_grid,\n                           scoring='accuracy',\n                           cv=5,\n                           n_jobs=-1,\n                           verbose=1)\n\n# Fit grid search on the training data\ngrid_search.fit(X_train_processed, y_train)\n\n# Print the best parameters and cross-validation accuracy\nprint(\"Best parameters:\", grid_search.best_params_)\nprint(\"Best CV accuracy:\", grid_search.best_score_)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission\n\nLightBGM implements gradient boosting descision trees which are known to [excel on structured (tabular) data](https://medium.com/geekculture/why-tree-based-models-beat-deep-learning-on-tabular-data-fcad692b1456). Hence the final model comes to no surprise.\n\nI would like to extend my gratitude to the mentors of MLP Project (Jan term 2025) and the mentors of MLP theory course, MLF theory course and TDS at IITM (BSc Data Science)","metadata":{}},{"cell_type":"code","source":"X_test_processed = preprocessor.transform(test)\n\ngrid_search.fit(X_train_processed, y_train)\n\nbest_model = grid_search.best_estimator_\n\ntest_preds = best_model.predict(X_test_processed)\n\nsubmission = pd.DataFrame({\n    'id': test.index,  \n    'target': test_preds\n})\n\n# Save to CSV\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T08:27:20.149946Z","iopub.execute_input":"2025-03-21T08:27:20.150345Z","iopub.status.idle":"2025-03-21T08:27:20.158251Z","shell.execute_reply.started":"2025-03-21T08:27:20.150312Z","shell.execute_reply":"2025-03-21T08:27:20.156981Z"}},"outputs":[],"execution_count":null}]}